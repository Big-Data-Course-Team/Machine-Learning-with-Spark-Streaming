# Call general Spark NLP transformers and concepts
from sparknlp.base import *
# Call all annotators provided by Spark NLP
from sparknlp.annotator import *
from pyspark.ml import Pipeline

'''
 All ML models are built using:
	- Estimators (with method fit())
	- Transformers (result of a fitted process and trains data)
 Pipelines: combine multiple estimators and transformers in a single workflow
 Annotation: basic result of a Spark NLP operation; object generated by annotators after a transform process
 	- Annotator Approaches: estimators - fit(); produce annotator models
 	- Annotator Models: transformers - transform(); Input-DF, Output-Append result as new column to DF.
'''

# Get raw data annotated - to type Document
documentAssembler = DocumentAssembler() \
    .setInputCol("Tweet") \
    .setOutputCol("document")


# Outputs results onto an array
finisher = Finisher() \
    .setInputCols(["token"]) \
    .setIncludeMetadata(True)
    
# Setup the pipeline
pipeline = Pipeline() \
    .setStages([
        documentAssembler,
        sentenceDetector,
        regexTokenizer,
        finisher
    ])
